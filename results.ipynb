{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to reproduce all the numeric results in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dataloader import get_dataloader, walk_through_dir\n",
    "from models import SimpleSegmentationModel, SegmentationModel, pretrained_UNet\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    all_preds = []  # List to store all predictions\n",
    "    all_labels = []  # List to store all ground truth labels\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, masks in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Convert outputs to binary predictions (threshold at 0.5 for binary classification)\n",
    "            binary_outputs = (outputs > 0.5).float()\n",
    "\n",
    "            # Flatten the outputs and masks for evaluation\n",
    "            binary_outputs_np = binary_outputs.cpu().numpy().flatten()\n",
    "            masks_np = masks.cpu().numpy().flatten()\n",
    "\n",
    "            # Collect predictions and true labels for metrics calculation\n",
    "            all_preds.extend(binary_outputs_np)\n",
    "            all_labels.extend(masks_np)\n",
    "\n",
    "    # Calculate average loss\n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    \n",
    "    # Ensure that labels and predictions are both binary (0 or 1)\n",
    "    all_preds = [int(x) for x in all_preds]  # Convert predictions to integer binary\n",
    "    all_labels = [int(x) for x in all_labels]  # Convert labels to integer binary\n",
    "\n",
    "    # Compute Precision, Recall, and F1 score\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    # Compute Dice Score\n",
    "    intersection = np.sum(np.array(all_preds) * np.array(all_labels))\n",
    "    dice_score = (2.0 * intersection) / (np.sum(all_preds) + np.sum(all_labels) + 1e-8)\n",
    "\n",
    "    # Compute IoU (Intersection over Union)\n",
    "    union = np.sum((np.array(all_preds) + np.array(all_labels)) > 0)\n",
    "    iou = intersection / (union + 1e-8)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Average Loss on Test Set: {avg_loss:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Dice Score: {dice_score:.4f}\")\n",
    "    print(f\"IoU: {iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/zhome/70/5/14854/nobackup/deeplearningf24/forcebiology/data/\"\n",
    "image_dirs = [data_path + 'brightfield/Alexa488_Fibroblasts_well1_50locations',\n",
    "                data_path + 'brightfield/Alexa488_Fibroblasts_well2_200locations',\n",
    "                data_path + 'brightfield/Alexa488_Fibroblasts_well3_200locations',\n",
    "                data_path + 'brightfield/Alexa488_Fibroblasts_well4_225locations',\n",
    "                data_path + 'brightfield/Alexa488_Fibroblasts_well5_225locations',\n",
    "                data_path + 'brightfield/Alexa488_Fibroblasts_well6_135locations',\n",
    "                data_path + 'brightfield/Alexa488_Fibroblasts_well7_135locations']\n",
    "mask_dir = data_path + 'masks'\n",
    "\n",
    "def set_seed(seed=111):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "mask_transform = data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(channels pth_model, model_name):\n",
    "    # Get the dataloaders with optional channel selection\n",
    "    train_dataloader, val_dataloader, test_dataloader = get_dataloader(\n",
    "        image_dirs, \n",
    "        mask_dir, \n",
    "        data_transform, \n",
    "        mask_transform, \n",
    "        display_sample=False, \n",
    "        train_percentage=1.0, \n",
    "        channel_indices=channels\n",
    "    )\n",
    "\n",
    "    # Initialize the device\n",
    "    #Â device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = \"cpu\"\n",
    "    criterion = nn.BCELoss()\n",
    "    current_directory = os.getcwd()\n",
    "    model_save_path = os.path.join(current_directory, pth_model)\n",
    "\n",
    "    # Load the model\n",
    "    if model_name == \"Simple\":\n",
    "        model = SimpleSegmentationModel().to(device)\n",
    "    elif model_name == \"UNet\":\n",
    "        model = SegmentationModel(channels=len(channels)).to(device)\n",
    "    elif model_name == \"Pretrained\":\n",
    "        model = pretrained_UNet().to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_save_path, map_location=torch.device('cpu')))\n",
    "    print(f\"Model {pth_model} loaded successfully.\")\n",
    "\n",
    "    return model, test_dataloader, criterion, device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evaluation for different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inital UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "augmentation = False\n",
    "fft = False\n",
    "pth_model = 'models/segmentation_model_UNet_train100%_channels0_1_2_3_4_5_6_7_8_9_10.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, augmentation, fft, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "augmentation = False\n",
    "fft = False\n",
    "pth_model = 'models/segmentation_model_Pretrained.pth'\n",
    "model_name = 'Pretrained'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, augmentation, fft, pth_model, model_name)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on train percentaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "pth_model = 'models/segmentation_model_UNet_train20%_channels0_1_2_3_4_5_6_7_8_9_10.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "pth_model = 'models/segmentation_model_UNet_train40%_channels0_1_2_3_4_5_6_7_8_9_10.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "pth_model = 'models/segmentation_model_UNet_train60%_channels0_1_2_3_4_5_6_7_8_9_10.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "pth_model = 'models/segmentation_model_UNet_train80%_channels0_1_2_3_4_5_6_7_8_9_10.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With augmentation and fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "pth_model = 'models/segmentation_model_UNet_train80%_channels0_1_2_3_4_5_6_7_8_9_10_augmentation_fft.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With different channels\n",
    "Channels based on GradCAM channel importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [1]\n",
    "pth_model = 'models/segmentation_model_UNet_train100%_channels1.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [1, 7]\n",
    "pth_model = 'models/segmentation_model_UNet_train100%_channels1_7.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [1, 7, 2, 4, 3, 9, 6]\n",
    "pth_model = 'models/segmentation_model_UNet_train100%_channels1_7_2_4_3_9_6.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channels chosen based on focal position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [6]\n",
    "pth_model = 'models/segmentation_model_UNet_train100%_channels6.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [5, 6, 7]\n",
    "pth_model = 'models/segmentation_model_UNet_train100%_channels5_6_7.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [4, 5, 6, 7, 8]\n",
    "pth_model = 'models/segmentation_model_UNet_train100%_channels4_5_6_7_8.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [3, 4, 5, 6, 7, 8, 9]\n",
    "pth_model = 'models/segmentation_model_UNet_train100%_channels3_4_5_6_7_8_9.pth'\n",
    "model_name = 'UNet'\n",
    "\n",
    "model, test_dataloader, criterion, device = run_model(channels, pth_model, model_name)\n",
    "evaluate_model(model, test_dataloader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
